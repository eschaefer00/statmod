{
 "cells": [
  {
   "cell_type": "code",
   "id": "7d4c6554",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T06:04:24.193889Z",
     "start_time": "2024-06-18T06:04:24.189891Z"
    }
   },
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "6ae8d491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T06:04:24.245891Z",
     "start_time": "2024-06-18T06:04:24.240889Z"
    }
   },
   "source": [
    "t = np.array([1, 2, 4, 7, 12, 21, 35, 59, 99, 200])\n",
    "nt = len(t)\n",
    "ns = 4\n",
    "# it might be useful to have tmat and k in the same shape, thus there happens some reshaping\n",
    "tmat = np.repeat(t, ns).reshape(nt, -1).T\n",
    "k = np.ma.masked_values([18, 18, 16, 13, 9, 6, 4, 4, 4, -999,\n",
    "                          17, 13,  9,  6, 4, 4, 4, 4, 4, -999,\n",
    "                          14, 10,  6,  4, 4, 4, 4, 4, 4, -999,\n",
    "                          -999, -999, -999, -999, -999, -999, -999, -999, -999, -999], \n",
    "                          value=-999).reshape(ns,-1)\n",
    "n = 18"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "ecec7c73-7812-4296-bc4d-e1f790f8c16f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T06:04:24.251890Z",
     "start_time": "2024-06-18T06:04:24.246889Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "This function takes the result of `pm.sample_posterior_predictive` as input\n",
    "and plots the comparison between the posterior predictive and the human behavior\n",
    "\"\"\"\n",
    "def plot_predict(ppc):\n",
    "    predict_trace = ppc[\"posterior_predictive\"][\"kij\"]\n",
    "    pt_values = predict_trace.values.reshape(-1, 4, 10)\n",
    "    _, axes = plt.subplots(2, 2, figsize=(6,6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for s in range(ns):\n",
    "        ax = axes[s]\n",
    "        for ts in range(nt):\n",
    "            sizes, positions = np.histogram(pt_values[:, s, ts], bins=np.arange(0, n+1), density=True)\n",
    "            ax.scatter([ts] * len(sizes), positions[1:], s=sizes*100, marker=\"s\", c=\"k\")\n",
    "        ax.plot(k[s, :], c=\"r\")\n",
    "    \n",
    "        ax.set(\n",
    "            xlabel=\"Retention Time Span\",\n",
    "            ylabel=\"Remembered Items\",\n",
    "            xticks=range(len(t)),\n",
    "            xticklabels=t\n",
    "        )\n",
    "    \n",
    "    plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "d52e02d4",
   "metadata": {},
   "source": [
    "### 3.1 Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "7b07e497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T06:04:26.857408Z",
     "start_time": "2024-06-18T06:04:24.257890Z"
    }
   },
   "source": [
    "simple_model = pm.Model()\n",
    "\n",
    "with simple_model:\n",
    "    \n",
    "    # prior\n",
    "    alpha = pm.Beta('alpha', alpha=1, beta=1, )\n",
    "    beta = pm.Beta('beta', alpha=1, beta=1, )\n",
    "\n",
    "    # parameter transformation // determine theta\n",
    "    theta_t = pm.Deterministic('theta_t', pm.math.clip(pm.math.exp(-alpha * tmat) + beta, 0, 1))\n",
    "\n",
    "    # likelihood\n",
    "    k_observed = pm.Binomial('k_observed', n=n, p=theta_t, observed=k)\n",
    "\n",
    "    # sampling: we have set some initializiations that might help\n",
    "    trace1 = pm.sample(2000, tune=2000, target_accept=0.99, initvals={'alpha': 0.5, 'beta': 0.25})\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elisa Schäfer\\PycharmProjects\\TU\\statmod\\statmod\\lib\\site-packages\\pymc\\model\\core.py:1356: ImputationWarning: Data in k_observed contains missing values and will be automatically imputed from the sampling distribution.\n",
      "  warnings.warn(impute_message, ImputationWarning)\n",
      "C:\\Users\\Elisa Schäfer\\PycharmProjects\\TU\\statmod\\statmod\\lib\\site-packages\\pytensor\\tensor\\elemwise.py:753: RuntimeWarning: divide by zero encountered in log\n",
      "  variables = ufunc(*ufunc_args, **ufunc_kwargs)\n",
      "C:\\Users\\Elisa Schäfer\\PycharmProjects\\TU\\statmod\\statmod\\lib\\site-packages\\pytensor\\tensor\\elemwise.py:753: RuntimeWarning: invalid value encountered in subtract\n",
      "  variables = ufunc(*ufunc_args, **ufunc_kwargs)\n",
      "C:\\Users\\Elisa Schäfer\\PycharmProjects\\TU\\statmod\\statmod\\lib\\site-packages\\pytensor\\tensor\\elemwise.py:753: RuntimeWarning: divide by zero encountered in log\n",
      "  variables = ufunc(*ufunc_args, **ufunc_kwargs)\n",
      "C:\\Users\\Elisa Schäfer\\PycharmProjects\\TU\\statmod\\statmod\\lib\\site-packages\\pytensor\\tensor\\elemwise.py:753: RuntimeWarning: divide by zero encountered in log\n",
      "  variables = ufunc(*ufunc_args, **ufunc_kwargs)\n",
      "C:\\Users\\Elisa Schäfer\\PycharmProjects\\TU\\statmod\\statmod\\lib\\site-packages\\pytensor\\tensor\\elemwise.py:753: RuntimeWarning: invalid value encountered in subtract\n",
      "  variables = ufunc(*ufunc_args, **ufunc_kwargs)\n"
     ]
    },
    {
     "ename": "SamplingError",
     "evalue": "Initial evaluation of model at starting point failed!\nStarting values:\n{'alpha_logodds__': array(0.), 'beta_logodds__': array(-1.09861229), 'k_observed_unobserved': array([ 4,  4,  4, 15, 11,  7,  5,  5,  5,  5,  5,  4,  4], dtype=int64)}\n\nLogp initial evaluation results:\n{'alpha': -inf, 'beta': -1.58, 'k_observed_unobserved': -20.53, 'k_observed_observed': -72.81}\nYou can call `model.debug()` for more details.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mSamplingError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 16\u001B[0m\n\u001B[0;32m     13\u001B[0m k_observed \u001B[38;5;241m=\u001B[39m pm\u001B[38;5;241m.\u001B[39mBinomial(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mk_observed\u001B[39m\u001B[38;5;124m'\u001B[39m, n\u001B[38;5;241m=\u001B[39mn, p\u001B[38;5;241m=\u001B[39mtheta_t, observed\u001B[38;5;241m=\u001B[39mk)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# sampling: we have set some initializiations that might help\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m trace1 \u001B[38;5;241m=\u001B[39m \u001B[43mpm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtune\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_accept\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.99\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitvals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43malpha\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbeta\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.25\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\TU\\statmod\\statmod\\lib\\site-packages\\pymc\\sampling\\mcmc.py:771\u001B[0m, in \u001B[0;36msample\u001B[1;34m(draws, tune, chains, cores, random_seed, progressbar, progressbar_theme, step, var_names, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, blas_cores, model, **kwargs)\u001B[0m\n\u001B[0;32m    769\u001B[0m ip: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray]\n\u001B[0;32m    770\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ip \u001B[38;5;129;01min\u001B[39;00m initial_points:\n\u001B[1;32m--> 771\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_start_vals\u001B[49m\u001B[43m(\u001B[49m\u001B[43mip\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    772\u001B[0m     _check_start_shape(model, ip)\n\u001B[0;32m    774\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m var_names \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\TU\\statmod\\statmod\\lib\\site-packages\\pymc\\model\\core.py:1777\u001B[0m, in \u001B[0;36mModel.check_start_vals\u001B[1;34m(self, start)\u001B[0m\n\u001B[0;32m   1774\u001B[0m initial_eval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpoint_logps(point\u001B[38;5;241m=\u001B[39melem)\n\u001B[0;32m   1776\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(np\u001B[38;5;241m.\u001B[39misfinite(v) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m initial_eval\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m-> 1777\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SamplingError(\n\u001B[0;32m   1778\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInitial evaluation of model at starting point failed!\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1779\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting values:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00melem\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1780\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLogp initial evaluation results:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00minitial_eval\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1781\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou can call `model.debug()` for more details.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1782\u001B[0m     )\n",
      "\u001B[1;31mSamplingError\u001B[0m: Initial evaluation of model at starting point failed!\nStarting values:\n{'alpha_logodds__': array(0.), 'beta_logodds__': array(-1.09861229), 'k_observed_unobserved': array([ 4,  4,  4, 15, 11,  7,  5,  5,  5,  5,  5,  4,  4], dtype=int64)}\n\nLogp initial evaluation results:\n{'alpha': -inf, 'beta': -1.58, 'k_observed_unobserved': -20.53, 'k_observed_observed': -72.81}\nYou can call `model.debug()` for more details."
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "75e10c69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T06:04:26.858408Z",
     "start_time": "2024-06-18T06:04:26.858408Z"
    }
   },
   "source": [
    "# Plot the posterior for alpha, beta and all theta's\n",
    "az.plot_trace(trace1, var_names=['alpha', 'beta'])\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cd680bad1133b125",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fd222871",
   "metadata": {},
   "source": [
    "# summarize the posterior distribution\n",
    "summary = az.summary(trace1, var_names=['alpha', 'beta'])\n",
    "print(summary)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "az.plot_trace(trace1, var_names=['theta_t'], compact=True)\n",
    "plt.show()"
   ],
   "id": "adba6bd9d2a91b59",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2a73e4d5-a894-4212-9b99-bbac4ed58860",
   "metadata": {},
   "source": [
    "# Posterior Predictive Sampling\n",
    "# with simple_model:\n",
    "    # ppc = pm.sample_posterior_predictive(trace1)\n",
    "\n",
    "# Formatting ppc for plot_predict function\n",
    "# formatted_ppc = {\"posterior_predictive\": {\"kij\": ppc[\"k_observed\"]}}\n",
    "\n",
    "# Plotting\n",
    "plot_predict(pm.sample_posterior_predictive(trace1,simple_model))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Explanation and Analysis:\n",
    "\n",
    "    Priors:\n",
    "        We use normal distributions for αα and ββ centered around 0.5 and 0.25, respectively, with standard deviations of 0.5 and 0.25.\n",
    "\n",
    "    Parameter Transformation:\n",
    "        θtθt​ is calculated using the given formula, with values clipped to be between 0 and 1.\n",
    "\n",
    "    Likelihood:\n",
    "        The likelihood of observing the data kk is modeled using a Binomial distribution, where the probability of success is given by θtθt​.\n",
    "\n",
    "    Sampling:\n",
    "        We run the MCMC sampler to obtain the posterior distributions for the parameters.\n",
    "\n",
    "    Posterior Predictive Checks:\n",
    "        Posterior predictive samples are generated and compared to the observed data to assess the model's performance.\n",
    "\n",
    "    Plotting and Interpretation:\n",
    "        The plot_predict function visualizes the comparison between the posterior predictive samples and the actual observed data, helping us understand how well the model predicts and generalizes.\n",
    "\n",
    "# Results Interpretation:\n",
    "\n",
    "    The posterior distributions of αα and ββ provide insights into the decay rate and baseline memory retention.\n",
    "    The theta_t distributions help visualize the expected probability of remembering items at different time points.\n",
    "    The posterior predictive checks allow us to see how well the model's predictions align with the observed data, including both prediction (missing values at time point 200) and generalization (missing values for subject 4).\n",
    "\n",
    "Overall, this implementation provides a comprehensive analysis of the memory retention model using PyMC."
   ],
   "id": "9b547f99da4d1d2d"
  },
  {
   "cell_type": "markdown",
   "id": "00bd5209",
   "metadata": {},
   "source": [
    "### 3.2 Individual Differences"
   ]
  },
  {
   "cell_type": "code",
   "id": "30134ac3",
   "metadata": {},
   "source": [
    "individual_model = pm.Model()\n",
    "\n",
    "with individual_model:\n",
    "    # Priors\n",
    "    alpha = pm.Normal('alpha', mu=0.5, sigma=0.5, shape=ns)\n",
    "    beta = pm.Normal('beta', mu=0.25, sigma=0.25, shape=ns)\n",
    "\n",
    "    # Parameter transformation: determine theta\n",
    "    theta_t = pm.Deterministic('theta_t', pm.math.clip(pm.math.exp(-alpha[:, None] * tmat) + beta[:, None], 0, 1))\n",
    "\n",
    "    # Likelihood\n",
    "    k_observed = pm.Binomial('k_observed', n=n, p=theta_t, observed=k)\n",
    "    \n",
    "    # Sampling with correct initial values shape\n",
    "    trace2 = pm.sample(1000, target_accept=0.99, \n",
    "                       initvals={'alpha': np.array([0.5] * 4), 'beta': np.array([0.25] * 4)})\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "25d8511c",
   "metadata": {},
   "source": [
    "# Plot the posterior for alpha, beta and all theta's\n",
    "az.plot_trace(trace2, var_names=['alpha', 'beta'])\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a8921ded",
   "metadata": {},
   "source": [
    "# summarize the posterior distribution\n",
    "summary = az.summary(trace2, var_names=['alpha', 'beta'])\n",
    "print(summary)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "az.plot_trace(trace2, var_names=['theta_t'], compact=True)\n",
    "plt.show()"
   ],
   "id": "e6b07e6cbfc569ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1ff4723c-6166-42ac-ad40-7ae15fcd297e",
   "metadata": {},
   "source": [
    "# Compare posterior predictive checks with human behavior\n",
    "with individual_model:\n",
    "    ppc = pm.sample_posterior_predictive(trace2, var_names=[\"k_observed\"])\n",
    "\n",
    "# Rename the posterior predictive samples to match what the plot_predict function expects\n",
    "ppc = {\"posterior_predictive\": {\"kij\": ppc[\"k_observed\"]}}\n",
    "\n",
    "plot_predict(ppc)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Advantages and Disadvantages of the Individual Differences Model\n",
    "## Advantages:\n",
    "\n",
    "    Captures Individual Variability: This model allows for individual differences in memory retention, making it more realistic and capable of capturing subject-specific nuances.\n",
    "    Better Fit: By accounting for individual differences, the model is likely to fit the data better and provide more accurate predictions for each subject.\n",
    "\n",
    "## Disadvantages:\n",
    "\n",
    "    Increased Complexity: The model is more complex, requiring more parameters and possibly more computational resources for fitting.\n",
    "    Overfitting Risk: With more parameters, there's a higher risk of overfitting, especially if the sample size is small relative to the number of parameters.\n",
    "\n",
    "## Changes in Inferences and Posterior Predictive Checks\n",
    "\n",
    "    More Accurate Predictions: The posterior predictive checks are expected to be more accurate as the model now captures individual differences.\n",
    "    Different Parameter Estimates: The individual αα and ββ estimates for each subject may provide insights into how memory retention varies between subjects.\n",
    "    Improved Generalization: By modeling individual differences, the model may generalize better to new subjects or new data points."
   ],
   "id": "989b2e3f5ec93e08"
  },
  {
   "cell_type": "markdown",
   "id": "06e674dc",
   "metadata": {},
   "source": [
    "### 3.3 Hierarchical Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "db2ae64b",
   "metadata": {},
   "source": [
    "hierarchical_model = pm.Model()\n",
    "\n",
    "with hierarchical_model:\n",
    "    # Hyperpriors\n",
    "    mu_alpha = pm.Normal('mu_alpha', mu=0.5, sigma=0.5)\n",
    "    sigma_alpha = pm.HalfNormal('sigma_alpha', sigma=0.5)\n",
    "    \n",
    "    mu_beta = pm.Normal('mu_beta', mu=0.25, sigma=0.25)\n",
    "    sigma_beta = pm.HalfNormal('sigma_beta', sigma=0.25)\n",
    "    \n",
    "    # Priors for each subject\n",
    "    alpha = pm.Normal('alpha', mu=mu_alpha, sigma=sigma_alpha, shape=ns)\n",
    "    beta = pm.Normal('beta', mu=mu_beta, sigma=sigma_beta, shape=ns)\n",
    "    \n",
    "    # Parameter transformation: calculate theta\n",
    "    theta_t = pm.Deterministic('theta_t', pm.math.clip(pm.math.exp(-alpha[:, None] * tmat) + beta[:, None], 0, 1))\n",
    "    \n",
    "    # Likelihood\n",
    "    k_observed = pm.Binomial('k_observed', n=n, p=theta_t, observed=k)\n",
    "    \n",
    "    # Sampling with correct initial values shape\n",
    "    trace3 = pm.sample(1000, target_accept=0.99, \n",
    "                       initvals={'alpha': np.array([0.5] * 4), 'beta': np.array([0.25] * 4)})\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5207e3a9",
   "metadata": {},
   "source": [
    "# Plot the posterior for alpha, beta and all theta's\n",
    "az.plot_trace(trace3, var_names=['alpha', 'beta'])\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "958cdc1f-0ab2-4fbd-95eb-f458028fb109",
   "metadata": {},
   "source": [
    "# summarize the posterior distribution\n",
    "summary = az.summary(trace3, var_names=['alpha', 'beta'])\n",
    "print(summary)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "az.plot_trace(trace3, var_names=['theta_t'], compact=True)\n",
    "plt.show()"
   ],
   "id": "17bc98bf753adc00",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b083d986-615e-4e38-a38d-cee8d83a109e",
   "metadata": {},
   "source": [
    "# Compare posterior predictive checks with human behavior\n",
    "with hierarchical_model:\n",
    "    ppc = pm.sample_posterior_predictive(trace3, var_names=[\"k_observed\"])\n",
    "\n",
    "# Rename the posterior predictive samples to match what the plot_predict function expects\n",
    "ppc = {\"posterior_predictive\": {\"kij\": ppc[\"k_observed\"]}}\n",
    "\n",
    "plot_predict(ppc)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Advantages and Disadvantages of the Hierarchical Model\n",
    "## Advantages:\n",
    "\n",
    "    Better Handling of Individual Differences: The hierarchical model allows for individual differences while also borrowing strength across subjects through the hyper-priors.\n",
    "    Regularization: By estimating hyper-priors, the model introduces regularization, potentially preventing overfitting by constraining individual parameter estimates.\n",
    "    Improved Generalization: This model can generalize better to new subjects as it captures population-level trends through the hyper-priors.\n",
    "\n",
    "## Disadvantages:\n",
    "\n",
    "    Increased Complexity: The hierarchical model is more complex, requiring more computational resources and possibly longer convergence times.\n",
    "    Difficult to Specify Hyper-priors: Choosing appropriate hyper-priors can be challenging and may require domain knowledge or additional data.\n",
    "\n",
    "## Changes in Inferences and Posterior Predictive Checks\n",
    "\n",
    "    More Accurate Posterior Estimates: The posterior estimates for αα and ββ should be more accurate as the model now accounts for both individual differences and population-level trends.\n",
    "    Improved Predictive Performance: The posterior predictive checks are expected to improve as the model leverages both individual and group-level information.\n",
    "    Insights into Population Trends: The hyper-priors provide insights into the overall population trends, which can be useful for understanding general memory retention behavior.\n",
    "\n",
    "This hierarchical approach offers a balance between capturing individual differences and leveraging population-level information, leading to potentially more robust and generalizable inferences."
   ],
   "id": "19614de4789985ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "65ad9bab371a7079"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
